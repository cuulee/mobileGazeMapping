@Article{Lowe2004,
    author = "Lowe, David G.",
    title = "Distinctive Image Features from Scale-Invariant Keypoints",
    journal = "International Journal of Computer Vision",
    year = "2004",
    month = "Nov",
    day = "01",
    volume = "60",
    number = "2",
    pages = "91--110",
    abstract="This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.",
    issn = "1573-1405",
    doi = "10.1023/B:VISI.0000029664.99615.94",
    url = "https://doi.org/10.1023/B:VISI.0000029664.99615.94"
}
@conference{visapp09,
    author = {Marius Muja and David G. Lowe},
    title = {FAST APPROXIMATE NEAREST NEIGHBORS WITH AUTOMATIC ALGORITHM CONFIGURATION},
    booktitle = {Proceedings of the Fourth International Conference on Computer Vision Theory and Applications - Volume 1: VISAPP, (VISIGRAPP 2009)},
    year = {2009},
    pages = {331-340},
    publisher = {SciTePress},
    organization = {INSTICC},
    doi = {10.5220/0001787803310340},
    isbn = {978-989-8111-69-2},
}
@article{opencv_library,
    author = {Bradski, G.},
    citeulike-article-id = {2236121},
    journal = {Dr. Dobb's Journal of Software Tools},
    keywords = {bibtex-import},
    posted-at = {2008-01-15 19:21:54},
    priority = {4},
    title = {{The OpenCV Library}},
    year = {2000}
}
@article {MacInnes299925,
	author = {MacInnes, Jeff J and Iqbal, Shariq and Pearson, John and Johnson, Elizabeth N},
	title = {Wearable Eye-tracking for Research: Automated dynamic gaze mapping and accuracy/precision comparisons across devices},
	year = {2018},
	doi = {10.1101/299925},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Wearable eye-trackers offer exciting advantages over screen-based systems, but their use in research settings has been hindered by significant analytic challenges as well as a lack of published performance measures among competing devices on the market. In this article, we address both of these limitations. We describe (and make freely available) an automated analysis pipeline for mapping gaze data from an egocentric coordinate system (i.e. the wearable eye-tracker) to a fixed reference coordinate system (i.e. a target stimulus in the environment). This pipeline allows researchers to study aggregate viewing behavior on a 2D planar target stimulus without restricting the mobility of participants. We also designed a task to directly compare calibration accuracy and precision across 3 popular models of wearable eye-trackers: Pupil Labs 120Hz Binocular glasses, SMI ETG 2 glasses, and the Tobii Pro Glasses 2. Our task encompassed multiple viewing conditions selected to approximate distances and gaze angles typical for short- to mid-range viewing experiments. This work will promote and facilitate the use of wearable eye-trackers for research in naturalistic viewing experiments.},
	URL = {https://www.biorxiv.org/content/early/2018/06/28/299925},
	eprint = {https://www.biorxiv.org/content/early/2018/06/28/299925.full.pdf},
	journal = {bioRxiv}
}
